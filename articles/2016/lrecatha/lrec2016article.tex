\documentclass[10pt, a4paper]{article}
\let\eachwordone\itshape

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
\usepackage{gb4e}
\usepackage[hidelinks]{hyperref}

\usepackage{lrec2016}
\usepackage{multibib}
\newcites{languageresource}{Language Resources}
\usepackage{graphicx}
\bibliography{references.bib}     
\usepackage{todonotes}
\title{The Alaskan Athabascan Grammar Database}
\name{Sebastian Nordhoff, Siri Tuttle, Olga Lovick}

\address{Glottotopia, UAF, U Olga \\
         Berlin, Anchorage, A Olga \\
         sebastian.nordhoff@glottotopia.de, stuttle@, olga@lithophile	\\}


\abstract{
Each article must include an abstract of 150 to 200 words in Times
9 pt with interlinear spacing of 10 pt. The heading Abstract should be
centred, font Times 10 bold. This short abstract will also be used for
printing a Booklet of Abstracts containing the abstracts of all papers
presented at the Conference. \\ 
\newline 
\Keywords{keyword A, keyword B, keyword C} }



\hyphenation{Atha-bas-can
au-to-ma-ted
fri-ca-tive
cum-ber-some
Ta-na-na
Ko-yu-kon
Ta-na-cross
wide-spread
JQue-ry}

\begin{document}

\maketitleabstract


\section{Introduction}

This paper presents the Alaskan Athabascan Grammar Database (AAGD). 

The goal of this database is to make available comparable annotated grammar examples from all eleven Athabascan languages, drawn from texts, lexicons, grey literature and new fieldwork, accessible to researchers and community members. This poses a number of technical, but also conceptual challenges, which will be explored in this paper. 

\todo[inline]{
The submission would also benefit from an explanation of the term "grey
literature".
}

\section{Athabascan\\languages}
The Athabascan language family numbers 53 languages. Of those 11 are spoken in Alaska, with a total number of speakers of about 5300. These are:
Ahtna, 
Deg Hit’an, 
Dena’ina/Ta\-nai\-na, 
Gwich\-’in,
Hän, 
Holi\-ka\-chuk, 
\textbf{Koyukon}, 
\textbf{Lower Tanana}, 
Middle Tanana, 
Tanacross, 
\textbf{Upper Tanana}, 
Upper Kus\-ko\-kwim.	
Languages in bold are included in the first phase of the project.


As far as the orthography is concerned, the languages are written in the standard Latin alphabet with the following special characters: acute accents for tone, ogoneks for nasalization and <ł> for the lateral fricative. Most researchers use their own personal orthography, entailing that automated generalizations across languages are difficult. 


Athabascan languages are known for their complex morphology. An example is given in (\ref{ex:complex}). 

\ea\label{ex:complex}
{\itshape Neeghonoheedekkaanh}
\gll Nee- gho- no- h- {$\emptyset$}- ee- de- kkaanh\\
\textsc{term}- \textsc{ppost}- \textsc{rev}- \textsc{3pl.sbj}- \textsc{{$\emptyset$}.cnj}- \textsc{pfv}- \textsc{d.clf}- paddle.\textsc{pfv}\\
\glt `They paddle back to shore.'\\
(Koyukon)
\z

% As a consequence of the involved morphology, research on these languages has mainly centered on morphology and a bit of phonology, while syntax has received less attention.

It is received wisdom that \em an Athabascan verb can convery everything necessary for a sentence\em. This could be taken to suggest that \em a verb is a sentence\em, and hence that there is no further need for syntax. This claim has not been seriously advanced, but it is still the case that syntactic research in Athabascan languages remains a desideratum.

% Additionally, grammatical terminology used in descriptions also varies, so that matching of glosses from different sources is not trivial.

% The documentation status of the languages is detailed in Table \ref{}
% 
% \begin{sidewaystable}
%  \begin{tabular}{p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}p{2cm}p{1cm}p{1cm}}
%            & &&paper & &&&digital \\
% Language & lexicon & grammar & texts & words & lexicon & grammar & texts & words  & audio & video \\
% 
%  \hline
%   Ahtna& word list & full grammar & 10 & 10k & word list  & sketch grammar  & 0 & 0 & 1h & 1h \\
%  Deg Hit’an&dictionary & sketch grammar & 3& 750 words & word list & -- &  1 & 100 & 3h & 2h \\
%  Dena’ina/Tanaina& & & & & & & \\
%  Gwich’in& & & & & & & \\
%  Hän& & & & & & & \\
%  Holikachuk& & & & & & & \\
%  Koyukon& & & & & & & \\
%  Lower Tanana& & & & & & & \\
%  Middle Tanana& & & & & & & \\
%  Tanacross& & & & & & & \\
%  Upper Tanana& & & & & & & \\
%  Upper Kuskokwim & & & & & & & \\
%  \end{tabular}
% \end{sidewaystable}

\section{Use case}
The long term goal of this project is to make all textual data from the Athabascan languages in Ala\-ska digitally available, and to complement it for the languages where data is lacking and where collection is still possible. 
There is a vast amount of grey literature for those languages. Making this accessible to researchers and language learners is also a goal.

For the initial phase, three languages were chosen: Upper Tanana, Lower Tanana and Koyukon due to the quality of the available material and the possibility to fill gaps via further fieldwork.

As for the linguistic scope, a focus is on syntax, as this has been a neglected area of research for those languages in the past. This decision has no direct consequences on the data collection, but it does have influences on annotation of the data and on the supplementary pedagogical material to be provided. Furthermore, it means that morphological annotation does not have to be as detailed. This is crucial, as deep morphological annotation of the kind exemplified in (\ref{ex:complex}) would be too time-consuming.

The project is special in that it has two audiences: on the one hands academic researchers in linguistics, who can be assumed to have the relevant background in terminology and linguistic theory to make sense of the structure of the data provided. On the other hand, language teachers, who might have very good or reasonable knowledge of the language, but lack the training in language description to appreciate the difference between clitics and affixes for instance. Catering to these two audiences at once has been a constant challenge.

The main use case for the academic audience is a repository of sentences with good search functionalities. Next to string search, search by tags/ca\-te\-gories (e.g. ``contains past tense'' or ``contains negation'') and search by similarity should are provided. 

A further use case  is the creation of a model for grammatical comparison transferrable to other, unrelated, language families. 


%     creating comparative morphosyntactic data (storyboards, other elicitation tools)


For the language teacher audience, the main use case is the preparation of lessons. Here, access to examples particularly suited to illustrate a certain point is crucial. Teachers preparing a lesson on negation, for instance, should have access to relevant sentences illustrating the phenomenon under discussion. Ideally, the sentences should be sorted according to their accessibility: straightforward sentences should be separated from contrived ones presenting difficulties unrelated to the phenomenon to be addressed. 



%     support of undergraduate and graduate students
%     providing infrastructure to support teaching and learning and study of Alaska Native Languages in recognition of their official status, something about the Language Board (Walkie)

\section{Data collection}

\todo[inline]{It would be great to address the data collection issue: what are the different
sources? How is the data collected?}


\section{Requirements} 
The requirements for the software were identified as the following: 
\begin{itemize}
 \item all three languages should be represented 
 \item it should be possible to extend the platform to the other (Alaskan) Athabascan languages, and potentially any other language 
 \item there should be a generic way to import data in various linguistic formats 
 (%
ELAN,\footnote{\url{https://tla.mpi.nl/tools/tla-tools/elan/}} 
Toolbox,\footnote{\url{http://www-01.sil.org/computing/toolbox/}}
Typecraft,\footnote{\url{http://typecraft.org/tc2wiki/Main_Page}} 
Brat\footnote{\url{http://brat.nlplab.org/}})
 \item there should be a way to annotate the data after it is imported
 \item there should be a way to retrieve the data 
 \begin{itemize}
  \item full text search 
  \item category search 
  \item similarity search 
 \end{itemize}
 \item the platform should be usable for researchers 
 \item the platform should be usable for language teachers 
 \item it should be possible to single out certain examples as particularly well suited for a certain didactic point
 \item it should be possible to fix minor errors online 
 \item users should have the possibility to upload additional texts
 \item user management and security
 \item possibility to add prose texts explaining certain phenomena
\end{itemize}



% \section{Challenges}
% One issue this project has to deal with is the conflict between faithfulness to the original source and searchability. In particular, should orthography be standardized across documents (good for searchability and comparison) or not (faithfulness to the original source, no potentially distorting interpretation)? Analagous to the point of standardizing the spelling is the standardization of terminology. 

\section{Implementation}
\subsection{Import}
The project uses a toolchain with POIO\footnote{\url{https://github.com/cidles/poio-api}} as a hub (Figure \ref{fig:toolchain}). POIO takes a variety of input formats, among which  the ELAN format, which is now widespread in language documentation projects. Poio transforms all those input formats in to LAF/GrAF. This allows us to be agnostic of the actual input format and focus on the conversion of LAF/GrAF (\cite{IdeRomary2006,IdeSuderman2007}) into an XML format to import into SOLR.\footnote{\url{http://lucene.apache.org/solr/}} 

\begin{figure}[t]
% Define block styles
\tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=5em, text centered, rounded corners, minimum height=4em]
\tikzstyle{line} = [draw, -latex']
\tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]
\resizebox{.3\textheight}{!}{
\begin{tikzpicture}[node distance = 2cm, auto]
    % Place nodes
    \node [block] (poio) {POIO};
    \node [cloud, above left of=poio] (toolbox) {Toolbox};
    \node [cloud, above right of=toolbox, node distance=1.7cm](elan) {ELAN} ;
    \node [cloud, above right of=poio] (typecraft) {Typecraft};
    \node [cloud, above left of=typecraft, node distance=1.7cm] (brat) {Brat};
    \node [cloud, below of=poio, node distance=2cm] (lafgraf) {LAF/GrAF};
    \node [block, below of=lafgraf, node distance=2cm] (python) {Python converter};
    \node [cloud, below of=python, node distance=2cm] (solr) {SOLR XML};
    % Draw edges
    \path [line] (elan) -- (poio);
    \path [line] (brat) -- (poio);
    \path [line] (typecraft) -- (poio);
    \path [line] (toolbox) -- (poio);
    \path [line] (poio) -- (lafgraf);
    \path [line] (lafgraf) -- (python);
%     \path [line] (python) -- (update);
%     \path [line] (update) |- (poio);
    \path [line] (python) -- (solr);
    
\end{tikzpicture}
}
\caption{The conversion tool chain. files are give in red, programs are given in blue. Import of SOLR XML into SOLR store not shown.}
\label{fig:toolchain}
\end{figure}

We use a SOLR setup with only minimal changes to the schema and configuration provided in the example installation. In particular, we add 20 fields for the domains we use for annotation.

\subsection{Annotation}
Annotation is done by the linguists in the web frontend. After experimenting with the extended OCCULT ontology (990 concepts),\footnote{\url{https://github.com/Glottotopia/ontologies/tree/master/occult}} the linguists found it easier to settle for a smaller shallow domain specific ontology consisting of 180 concepts. There is a clear separation between formal concepts like  \texttt{\small sen\-tence\_type:monoclausal} and meaning-based concepts like \texttt{\small participant\_role:ex\-pe\-rien\-cer}, following \cite{Nordhoff2012fufomp}.

Annotation is done via AJAX and a small python script on the server, which validates the input, updates the SOLR store and returns the callback message. 

\todo[inline]{

Regarding the linguistic design choices, these seem to be a trade-off between
detail of description and time-efficiency. First, full morphological annotation
would be too time consuming, and the syntax of the languages are in any case
unexplored. Second, a smaller domain specific ontology with less than 200
concepts was preferred by the linguistic annotators rather than the initially
tested ontology with almost 1000 concepts.  Presumably, a smaller set of
categories is more easily acquired and applied, and will also ensure a more
efficient and also more consistent annotation.

While the benefits and drawbacks of these final decisions could have been
highlighted, there are certain limitations to the poster format. The authors
should, however, be prepared to address these issues during a system
demonstration.
}

\subsection{View}
The velocity\footnote{\url{http://velocity.apache.org/}} templates shipped with SOLR were completely rewritten, using JQuery.\footnote{\url{http://jquery.com/}}  and  Bootstrap.\footnote{\url{http://getbootstrap.com/}} 
A Moinmoin\footnote{\url{http://moinmo.in/}} wiki is used for documentation with AJAX-based pulling of examples from SOLR JSON output. 
   
   
\todo[inline]{
It would also be interesting to know
to what extent the requirement that the system should be able to "single out
certain examples as particularly well suited for a certain didactic point", is
satisfied.
}
\section{Justification for this implementation}  
The POIO hub allows for a uniform treatment of linguistic data regardless of actual input format. The SOLR store brings Lucene search capabilities out of the box and relieves us from the need to provide the search facilities ourselves. Given the rather small amount of data, there is no need to use a RDBMS in the backend; the SOLR XML store is sufficient for the amount of data handled here. The shallow ontology is a compromise between a more articulate ontology and user experience when annotating. 

\section{Availability}
The source code is available on github at {\small\url{https://github.com/Glottotopia/aagd}}.
The website is available at {\small\url{http://www.glottotopia.org/aagd}}.
The language data are still being curated and will be made available in due course. 
 
\section{Significance}
Athabascan languages present least-resourced languages. The tools typically found in LREC like treebanks, thesauri, wordnets and the like are far away for languages where even a full grammar and a comprehensive dictionary are lacking. Nevertheless, there are resources in these languages, and modern technological tools developed for the larger languages like LAF/GrAF or SOLR can be fruitfully applied to these languages, even if the use cases are very different. Furthermore, the POIO bridge between ELAN files common in language documentation and LAF/GrAF will allow for the integration of much material from language documentation projects into the global linguistic resource repositories. 
\onecolumn

% \printbibliography
\begin{figure}[t]
\includegraphics[width=\textwidth]{aagd.png}
\caption{Screenshot of the web frontend with a free text search, one fully visible example in the middle and faceted search on the right}
\end{figure}

\section{Reviewers' comments}
 parallel to Oral Sessions and therefore their length varies accordingly: during the session you will have the opportunity to describe your work and interact with interested conference participants. We request that you be at your Poster Session for the entire time slot. Further details for poster presenters will be detailed on the conference web site at due time.

Regarding technical details and requests about your Demo, please contact Sara Goggi at the following email address: lrec@ilc.cnr.it.

The conditions for your participation in a Poster Session at LREC are the following:

1) you confirm your participation by registering for LREC by March 10th, 2016 which is also the deadline for early registration with reduced fee (at least one author per paper MUST register);
AND
2) you submit the final version of your paper by March 10th, 2016 in compliance with the "Authors' Kit", which will be soon available on the LREC web site and will contain the Style sheet (and length) for the Proceedings.


You can upload your final manuscript at the following site:

    https://www.softconf.com/lrec2016/main/user/

You will be prompted to login to your START account and thus access to your User Console: from here choose "Your current Submission(s)" for accessing the list of your submission(s). Click on this submission id for submitting the final full paper.

This is the passcode associated with your submission:

			  1171X-F2G9D6A3D5
 
Online registration and hotel booking, conference details and local information about Portorož will be soon available at the LREC2016 website (http://lrec2016.lrec-conf.org/en/).
 
 
 

---------------------------------------------------------------------------
Comments
---------------------------------------------------------------------------

This paper is about the Alaskan Athabascan Grammar Database project. It
specifically addresses “technical and conceptual challenges”. In section 2
it briefly describes the languages that are considered and in section 3 the
different use cases are provided. As far as the reviewer could see, the
conceptual problems mainly relate to the fact that the database should be
useful to both academic researchers and language teachers and should allow for
different kinds of searches. Technical issues pertain to file formats,
implementation details and semantic annotation of the data. In section 4 the
requirements for the system are listed, the bullets are basically an unordered
list of both conceptual and technical requirements. Unfortunately, these
requirements are not explained or justified in any way. In section 5 the
implementation is addressed. It reads like a project report and contains jargon
and acronyms that might not be known to the larger audience of interested
readers. The description is not adequate and the authors should provide
justification for their technical choices versus various others that could have
been made. Section 6 addresses this aspect to a limited extent, but in the full
paper it would be necessary to do much more in justifying design and technical
choices in a systematic and scientific way. The authors should not allow the
technology layer to obfuscate their scientific approach. In the section (8) on
the significance of the paper, important matters are touched upon. It is the
hope of this reviewer that the full paper will do more to make this work
amenable to other similar projects and groups of researchers. This is,
according to the reviewer, the most useful aspect of this project. There are
thousands of under-resourced languages that might benefit from such or similar
systems. It is also commendable that it is openly available (section 7). The
screenshot in figure 2 should be discussed in some detail otherwise its
inclusion should be reconsidered. 

The structure of the paper: It would be beneficial to make the research problem
of the paper under review explicit, do a proper contextualisation, make the
methodology that is used to solve the specific problem clear and pay sufficient
attention to an evaluation and results of the system. It will also be important
to ensure that the paper is clear about the details of the system for the
benefit of the wider research community, interested in under-resourced
languages.  The language is good.

The paper lacks a suitable contextualisation in the form a review of related
work. It is this aspect that will elevate the current project report-like
presentation and approach to real scientific research, as expected from a
submission to a conference such as LREC. The very limited list of references
attests to the absence of any kind of literature review. In the full paper this
should be rectified.
 

% \nocite{*}
\section{Bibliographical References}
\label{main:ref}

\bibliographystyle{lrec2016}
\bibliography{xample}


\section{Language Resource References}
\label{lr:ref}
\bibliographystylelanguageresource{lrec2016}
\bibliographylanguageresource{xample}

\end{document}
